---
title: "p8105_hw6_mnt2130"
author: "Mindy Tran"
date: "2022-12-03"
output: github_document 
---

```{r setup, include=FALSE}
library(tidyverse)
library(viridis)
library(readr)
library(purrr)
library(patchwork)
library(dplyr)
library(modelr)
library(mgcv)
library (knitr)

knitr::opts_chunk$set(
	echo = TRUE,
	warning = FALSE,
	fig.width = 8, 
  fig.height = 6,
  out.width = "90%"
)

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d

theme_set(theme_minimal() + theme(legend.position = "bottom"))
```

### Problem 1

This code below obtains a distribution for $\hat{r}^2$ by drawing bootstrap samples; the a model to each andextracts the value I'm concerned with and summarizes it summarize. Here, we'll use `modelr::bootstrap` to draw the samples and `broom::glance` to produce `r.squared` values. 

```{r weather_df, cache = TRUE}
weather_df = 
  rnoaa::meteo_pull_monitors(
    c("USW00094728"),
    var = c("PRCP", "TMIN", "TMAX"), 
    date_min = "2017-01-01",
    date_max = "2017-12-31") %>%
  mutate(
    name = recode(id, USW00094728 = "CentralPark_NY"),
    tmin = tmin / 10,
    tmax = tmax / 10) %>%
  select(name, id, everything())
```


```{r}
weather_df %>% 
  modelr::bootstrap(n = 5000) %>% 
  mutate(
    models = map(strap, ~lm(tmax ~ tmin, data = .x) ),
    results = map(models, broom::glance)) %>% 
  select(-strap, -models) %>% 
  unnest(results) %>% 
  ggplot(aes(x = r.squared)) + geom_density() +
   labs(
    x = "R Squared Values",
    y = "Density",
    title = "Distribution of R-Squared estimates for Central Park Weather"
  ) 

```

Based on the results, the $\hat{r}^2$ value is high. The upper bound at 1 may be a cause for the generally skewed shape of the distribution. If we wanted to construct a confidence interval for $R^2$, we could take the 2.5% and 97.5% quantiles of the estimates across bootstrap samples. However, since the shape of the curve generated isn't symmetric, using the mean +/- 1.96 times the standard error would probably not work well in this case. 

This code produces a distribution for $\log(\beta_0 * \beta1)$ using a similar approach. 

```{r}
weather_df %>% 
  modelr::bootstrap(n = 1000) %>% 
  mutate(
    models = map(strap, ~lm(tmax ~ tmin, data = .x) ),
    results = map(models, broom::tidy)) %>% 
  select(-strap, -models) %>% 
  unnest(results) %>% 
  select(id = `.id`, term, estimate) %>% 
  pivot_wider(
    names_from = term, 
    values_from = estimate) %>% 
  rename(beta0 = `(Intercept)`, beta1 = tmin) %>% 
  mutate(log_b0b1 = log(beta0 * beta1)) %>% 
  ggplot(aes(x = log_b0b1)) + geom_density() +
  labs(
    x = "Log(B0*B1)",
    y = "Density",
    title = "Distribution of Log(B0*B1) estimates for Central Park Weather"
  ) 
```

As with $r^2$, this distribution is approximately normally distributed but also somewhat skewed and potentially has some outliers. 

### Problem 2
The following code chunk creates a city_state variable and a binary variable which indicates whether a homicide is resolved. It also omits the cities : Tulsa, Dallas, Phoenix, and Kansas which have missing data or data entry mistakes. We also limited our analyses to victims who were white or black and converted the variable victim_age to a numeric variable.

```{r homicide}

homicide_df = 
  read_csv("./data/homicide-data.csv", na = c("", "NA", "Unknown")) %>% 
  mutate(
    city_state = str_c(city, state, sep = "_"),
    resolution = case_when(
      disposition == "Closed without arrest" ~ 0,
      disposition == "Open/No arrest"        ~ 0,
      disposition == "Closed by arrest"      ~ 1,
    ),
    victim_age = as.numeric(victim_age)
  ) %>% 
  filter(!city_state %in% c("Tulsa_AL", "Dallas_TX", "Phoenix_AZ","Kansas City_MO"))  %>% 
  filter(victim_race %in% c("Black", "White")) %>% 
  filter(victim_sex %in% c("Male", "Female")) %>%  
  select(city_state, resolution, victim_age, victim_race, victim_sex)
```
The code below fits a logistic regression of resolved vs unresolved cases as the outcome and the age,sex, and race of the victims as predictors for the city of Baltimore and gives the adjusted odds ratio for solving homicides comparing male victims to female victims keeping all other variables fixed.
```{r baltimore}
baltimore_logistic = homicide_df %>% 
  filter(city_state == "Baltimore_MD") %>% 
glm(resolution ~ victim_age + victim_sex + victim_race, family = binomial() , data = .) %>%
  broom::tidy() %>% 
  mutate(
    odds_ratio = exp(estimate),
    ci_lower =  exp(estimate - 1.96 * std.error),
    ci_upper = exp(estimate + 1.96 * std.error)
  ) %>% 
  select(term, odds_ratio, starts_with("CI")) %>% 
  knitr::kable(digits = 3)
baltimore_logistic
```
The odds of a solved homicide for male victims is 0.426 (CI: 0.325, 0.558) times the odds of a solved homocide for female victims, controlling for age and race (White or Black). 

The code below fits a logistic regression of resolved vs unresolved cases as the outcome and the age,sex, and race of the victims as predictors for all cities in the dataset besides the 4 that we excluded in the data cleaning step. 

```{r all_cities}
models_results = homicide_df %>% 
  nest(data = -city_state) %>% 
  mutate(
    models = map(.x = data, ~ glm(resolution ~ victim_age + victim_sex + victim_race, data = .x, family = binomial())),
    results = map(models, broom::tidy)
  ) %>% 
  select(city_state, results) %>% 
  unnest(results) %>% 
  mutate(
    odds_ratio = exp(estimate),
    ci_lower =  exp(estimate - 1.96 * std.error),
    ci_upper = exp(estimate + 1.96 * std.error)
  ) %>% 
  select(term, odds_ratio, starts_with("CI"))
```
The code below create a plot that shows the estimated ORs and CIs for each city and organizes the cities according to estimated OR in ascending order. 

```{r plot, fig.width=8}
models_results %>% 
  filter(term == "victim_sexMale") %>% 
  mutate(city_state = fct_reorder(city_state, odds_ratio)) %>% 
  ggplot(aes(x = city_state, y = odds_ratio)) + 
  geom_point() + 
  geom_errorbar(aes(ymin = ci_lower, ymax = ci_upper)) +
 theme(axis.text.x = element_text(angle = 60, vjust = 1.0, hjust = 1), legend.position = "none") + 
 geom_hline(yintercept = 1, linetype="dashed", color = "red", linewidth = 0.5) +
  labs(
    title = "Odds Ratio of Homicide Case Resolution Status by City",
    subtitle  = "The odds ratio of case resolution comparing male victims to female victims, controlling for age & sex",
    y = "Odds Ratio",
    x = "City-States"
  )  
```

New York, NY has the lowest odds ratio of case resolution comparing male victims to female victims when controlling for age and sex. Alberqurque, NM has the highest odds ratio of case resolution when comparing male victims to female victims when controlling for age and sex, it also has the widest confidence interval indicating less reliability when interpreting this odds ratio estimate. For 15 states, the CI crosses the odds ratio null value of 1, indicating that the odds ratio generated by this model is not statistically significant for those states. 

### Problem 3
The following code chunk converts `babysex` to a  factor variable, converts both the variables father's and mother's race and labels it, converts mother's weight at delivery `delwt` to grams and `mheight` to cm and to keep the same units of measurements as the baby's. It also checks for missing values in the data, which there are none. 

```{r, message=F, warning=F}
baby_df = 
  read_csv("./data/birthweight.csv") %>%
  mutate(
    babysex = ifelse(babysex == 1, "male", "female") %>% as.factor(),
    frace = 
      case_when(
        frace == 1 ~ "White",
        frace == 2 ~ "Black",
        frace == 3 ~ "Asian",
        frace == 4 ~ "Puerto Rican",
        frace == 8 ~ "Other",
        frace == 9 ~ "Unknown",
        TRUE ~ ""
      ) %>% as.factor(),
    mrace = 
      case_when(
        mrace == 1 ~ "White",
        mrace == 2 ~ "Black",
        mrace == 3 ~ "Asian",
        mrace == 4 ~ "Puerto Rican",
        mrace == 8 ~ "Other",
        TRUE ~ ""
      ) %>% as.factor(),
    mrace = fct_relevel(mrace, "Other"),
    delwt = delwt*453.592 %>% round(2),
    mheight = mheight*2.54 %>% round(2)
  )

baby_df %>% skimr::skim() %>% select(skim_variable, n_missing, complete_rate) %>% knitr::kable()
```
## Initial model for birthweight

Using a hypothesis driven approach, a mother's age, weight, and height influence a baby's birthweight by proxy of genetics and existing literature that confirms the correlation. Therefore, we will build a model that fits a regression for the outcome of baby's birthweight using the mother's age, weight, and height as predictor variables to test the hypothesis that a baby's birthweight can be explained by a linear relationship with their mother's weight, while controlling for mother's age and height. 

The following code chunks tests for normality and linearity. 
```{r normality_linearity, message=F, warning=F, fig.width=8}
baby_df %>% 
  ggplot(aes(x = bwt)) +
  geom_histogram(binwidth = 5) + 
  labs(
    title = "Distribution of Babys' birthweight",
    y = "Count",
    x = "Birthweight (in grams)"
  )

bwt_delwt = baby_df %>% 
  ggplot(aes(x = bwt, y = delwt)) + 
  geom_smooth(se = F) + 
  geom_point(alpha = 0.1) + 
    labs(
    title = "Relationship between Birthweight and Mother's Weight at Delivery ",
    y = "Weight(grams)",
    x = "Birthweight (in grams)"
  )
bwt_mheight = baby_df %>% 
  ggplot(aes(x = bwt, y = mheight)) + 
  geom_smooth(se = F) + 
  geom_point(alpha = 0.1) + 
  labs(
    title = "Relationship between Birthweight and Mother's Height at Delivery ",
    y = "Height(cms)",
    x = "Birthweight (in grams)"
  )
bwt_momage = baby_df %>% 
  ggplot(aes(x = bwt, y = momage)) + 
  geom_point(alpha = 0.1) + 
  geom_smooth(se = F) + 
    labs(
    title = "Linear relationship between Birthweight and Mother's Age",
    y = "Age (years)",
    x = "Birthweight(in grams)"
  )
bwt_delwt / bwt_mheight / bwt_momage 

```
The histogram of the outcome baby's birth weight appears to follow a normal distribution. The relationship with the 3 predictor variables (mother's: weight, height and age) appear to be mostly linear relationship with the outcome of interest. 

The following code chunks fits a linear regression model and gives a summary of the model and estimates generated from the model. 
```{r}
lin_fit = 
  baby_df %>% 
lm(bwt ~ delwt + mheight + momage, data = .) 
```

**Model summary**
```{r}
lin_fit %>% 
  broom::glance() %>% 
  select(r.squared, adj.r.squared, sigma, statistic, p.value, df, nobs) %>% 
  knitr::kable(digits = 2)
```
It appears that the initial model, with an r.squared value of 0.1, only explains about 10% of the variation in the outcome variable: baby's `birthweight`

**Model estimates**
```{r}
lin_fit %>% 
  broom::tidy() %>% 
  knitr::kable(digits = 3)
```
The model has an adjusted r-squared value of 0.1, indicating that 10% of the variation in birth weight is explained by mother's weight, age, and height. 

The following code plots residuals vs. fitted values of the initial model

```{r}
baby_df %>%  
  add_residuals(lin_fit) %>% 
  add_predictions(lin_fit) %>% 
  ggplot(aes(x = pred, y = resid)) + 
  geom_point(alpha = .2) + 
  geom_smooth(se = F) + 
  labs(
    title = "Fitted Values Vs. Residuals (Initial Model)",
    x = "Model Predicted Birthweight",
    y = "Model Residuals"
  )
```
Based on the plot, the model seems uniformly distributed indicating that the variance of the errors is constant. However, there seems to be a curved trend in the model residuals in comparison to the birthweight, indicating that the model might have some shortcomings when trying to predict birthweight from our predictor variables. 


## Building two additional models 
Model 2 uses baby's birth weight as the outcome with birth length and gestational age as predictors 

Model 3 uses head circumference, length, sex, and an interaction term between the three predictors. 

```{r}
#model 2 -
baby_df %>% 
lm(bwt ~ blength + gaweeks, data = .) %>% 
  broom::tidy() %>% 
  knitr::kable(digits = 3)
#model 3 - 
baby_df %>% 
  lm(bwt ~ bhead + blength + babysex + bhead*blength*babysex, data = .) %>% 
  broom::tidy() %>% 
  knitr::kable(digits = 3)
```
This code chunk creates cross validation datasets, by splitting the original sample into 80% for training and 20% for testing, and creating 100 resample objects. 
```{r, warning=F, message=F}
cv_df = 
  crossv_mc(baby_df, 100)
#Convert the resample objects into a tibble
cv_df = cv_df %>% 
  mutate(
    train = map(train, as_tibble),
    test = map(test, as_tibble) 
    ) 
```


